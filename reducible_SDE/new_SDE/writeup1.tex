\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage[margin=1in]{geometry}

\begin{document}

\section{Multiscale SDEs}

Consider the SDEs
\begin{eqnarray}
dx &=& adt + dW_1\\
dy &=& -\frac{y}{\epsilon} dt + \frac{1}{\sqrt{\epsilon}} dW_2
\end{eqnarray}

So $y$ is a fast noise whose equilibrium measure is bounded and $\mathcal{O}(1)$

Let $y = \frac{z}{\sqrt{\epsilon}}$. Then
\begin{eqnarray}
dx &=& adt + dW_1\\
dz &=& -\frac{z}{\epsilon} dt +  dW_2
\end{eqnarray}

Therefore, now $z$ is a stochastic variable whose diffusion is unity, and whose equilibrium measure is $\mathcal{O}(\epsilon)$. 
%
Therefore, when $\epsilon \ll 1$, we have that $z \ll x$, and so we will recover $x$ only.

This falls into a more general form of SDE
\begin{equation}
dX = f(X) dt + dW
\end{equation}


\section{Covariance Estimation}

Given $X$ governed by the above SDE, we can sample ``bursts'' around a point $X_0$ in order to estimate the covariance. 

These clouds will be distributed 

\begin{equation}
X \sim \mathcal{N}\left( X_0, \delta t I \right)
\end{equation}

Then
$$\int_{-\infty}^{\infty} d\mu(X) = \begin{bmatrix} 1 \\ \vdots \\ 1\end{bmatrix}$$
$$\int_{-\infty}^{\infty} X d\mu(X) = X_0$$
$$\int_{-\infty}^{\infty} X X^T d\mu(X) - \left( \int_{-\infty}^{\infty} X d\mu(X) \right) \left( \int_{-\infty}^{\infty} X d\mu(X) \right)^T = \delta t I$$


We now consider a function $\mathbf{f}(X)$.
%
By Taylor expansion around $X=X_0$,
$$f^i(X) = f^i(X_0) + J^i(X_0) \cdot (X-X_0) + \frac{1}{2} (X-X_0)^T H^i(X_0) (X-X_0) + \dots$$
where $J^i$ is the Jacobian and $H^i$ is the Hessian. 

We can then approximate the first- and second-order moments 
\begin{eqnarray}
&&\int_{-\infty}^{\infty} f^i(X) d\mu(X) \\
&\approx &
 \int_{-\infty}^{\infty} \left[ f^i(X_0) + J^i(X_0) \cdot (X-X_0) +\frac{1}{2} (X-X_0)^T H^i(X_0) (X-X_0) \right] d\mu(X) \\
&=&\int_{-\infty}^{\infty} \left[ f^i(X_0) + \sum_k J^i_k(X_0) (X^k-X_0^k)  \right. \\
&& \left. + \frac{1}{2} \sum_k \sum_l H^i_{kl}(X_0) (X^k-X_0^k) (X^l-X_0^l) \right] d\mu(X) \\
&=& f^i(X_0) + \frac{1}{2} \sum_k H^i_{kk}(X_0) \delta t
\end{eqnarray}

\begin{eqnarray}
&&\int_{-\infty}^{\infty} f^i(X) f^j(X) d\mu(X) \\
&\approx &
\int_{-\infty}^{\infty} \left[ f^i(X_0) + J^i(X_0) \cdot (X(j)-X_0(j)) +\frac{1}{2} (X-X_0)^T H^i(X_0) (X-X_0) \right] \\
&& \left[ f^j(X_0) + J^j(X_0) \cdot (X-X_0) +\frac{1}{2} (X-X_0)^T H^j(X_0) (X-X_0) \right] d\mu(X) \\
&=& \int_{-\infty}^{\infty} \left[ f^i(X_0) f^j(X_0) + f^i(X_0) J^j(X_0) \cdot (X-X_0) \right. \\
&& + f^j(X_0) J^i(X_0) \cdot (X-X_0) \\
&& + \left( J^i(X_0) \cdot (X-X_0) \right) \left( J^j(X_0) \cdot (X-X_0) \right) \\
&& + f^i(X_0) \frac{1}{2} (X-X_0)^T H^j(X_0) (X-X_0) \\
&& \left. + f^j(X_0) \frac{1}{2} (X-X_0)^T H^i(X_0) (X-X_0) \right] d \mu(X) \\
&=& \int_{-\infty}^{\infty} \left[ f^i(X_0) f^j(X_0) + f^i(X_0) J^j(X_0) \cdot (X-X_0) \right. \\
&& + f^j(X_0) J^i(X_0) \cdot (X-X_0) \\
&& + \left( \sum_k J_k^i(X_0) (X^k - X_0^k) \right) \left( \sum_k J_k^j(X_0) (X^k - X_0^k) \right) \\
&& + f^i(X_0) \frac{1}{2} (X-X_0)^T H^j(X_0) (X-X_0) \\
&& \left. + f^j(X_0) \frac{1}{2} (X-X_0)^T H^i(X_0) (X-X_0) \right] d \mu(X)\\
&=& f^i(X_0) f^j(X_0) + \delta t \sum_k J_k^i(X_0) J_k^j(X_0)  \\
&& + f^i(X_0) \frac{1}{2} \sum_k H^j_{kk}(X_0) \delta t + f^j(X_0)\frac{1}{2} \sum_k H^i_{kk}(X_0) \delta t
\end{eqnarray}

We then write
\begin{eqnarray}
&& \hat{C}^{ij} = \\ 
&& \int_{-\infty}^{\infty} f^i(X) f^j(X) d\mu(X) - \left(\int_{-\infty}^{\infty} f^i(X) d\mu(X) \right) \left(\int_{-\infty}^{\infty} f^j(X) d\mu(X) \right) \\
&=& \left( f^i(X_0) f^j(X_0) + \delta t \sum_k J_k^i(X_0) J_k^j(X_0) \right. \\
&& \left. + f^i(X_0) \frac{1}{2} \sum_k H^j_{kk}(X_0) \delta t + f^j(X_0)\frac{1}{2} \sum_k H^i_{kk}(X_0) \delta t \right) \\
&& - \left(f^i(X_0) + \frac{1}{2} \sum_k H^i_{kk}(X_0) \delta t \right)
\left(f^j(X_0) + \frac{1}{2} \sum_k H^j_{kk}(X_0) \delta t \right) \\
&=& \delta t \sum_k J_k^i(X_0) J_k^j(X_0) - \frac{\delta t^2}{4} \left( \sum_k H^i_{kk}(X_0) \right) \left( \sum_k H^j_{kk}(X_0) \right)
\end{eqnarray}


In this equation, we see what we expect (a linear function of the variance, due to the gradient of $f$) and a second term arising from the curvature of $f$.
%
The importance of this second term increases as the curvature increases, and when the variance of the underlying process increases. 


For the specific case of our fast-slow SDEs:
$$f^1(X) = X(1)$$
$$f^2(X) = \frac{X(2)}{\sqrt{\epsilon}}$$
and so the covariance estimation is exact (provided we do not see the drift).

When we apply a nonlinear function, i.e., 
$$f^1(X) = \frac{X(2)}{\sqrt{\epsilon}} \cos \left(X(1)+\frac{X(2)}{\sqrt{\epsilon}} \right)$$
$$f^2(X) = \frac{X(2)}{\sqrt{\epsilon}} \sin \left(X(1)+\frac{X(2)}{\sqrt{\epsilon}} \right)$$

\begin{eqnarray}
H_{11}^1 &=& 
-\frac{X(2)}{\sqrt{\epsilon}} \cos \left(X(1)+\frac{X(2)}{\sqrt{\epsilon}} \right) \\
H_{22}^1 &=&
-\frac{X(2)}{\epsilon^{3/2}} \cos \left(X(1)+\frac{X(2)}{\sqrt{\epsilon}} \right) - \left( \frac{1}{\epsilon} + \frac{1}{\epsilon^{3/2}} \right) \sin \left(X(1)+\frac{X(2)}{\sqrt{\epsilon}} \right) 
\end{eqnarray}

TODO: finish nonlinear case/example


\section{Mahalanobis Distance}


We now approximate the distances $\| X_2 - X_1 \|$.
%
Let $g = f^{-1}$. 
%
Then
%
\begin{eqnarray}
X_2^i &=& X_1^i + \sum_j g_j^i (f(X_1)) (f^j(X_2) - f^j(X_1) ) \\
&&+ \frac{1}{2} \sum_{kl}  g^i_{kl} (f(X_1)) (f^k(X_2) - f^k(X_1))(f^l(X_2) - f^l(X_1)) \\
&&+ \frac{1}{6} \sum_{klm}  g^i_{klm} (f(X_1)) (f^k(X_2) - f^k(X_1))(f^l(X_2) - f^l(X_1)) (f^m(X_2) - f^m(X_1)) \\
&&+ \mathcal{O}( \|f(X_2) - f(X_1)\|^4 )
\end{eqnarray}

Then computing the distance
\begin{eqnarray}
&&\| X_2 - X_1 \|^2 \\
&=&  \sum_i (X_2^i - X_1^i)^2 \\
&=& \sum_i  \left( \sum_j g_j^i (f(X_1)) (f^j(X_2) - f^j(X_1) )  \right. \\
&&+ \frac{1}{2} \sum_{kl}  g^i_{kl} (f(X_1)) (f^k(X_2) - f^k(X_1))(f^l(X_2) - f^l(X_1)) \\
&&+ \frac{1}{6} \sum_{klm}  g^i_{klm} (f(X_1)) (f^k(X_2) - f^k(X_1))(f^l(X_2) - f^l(X_1)) (f^m(X_2) - f^m(X_1)) \\
&&+ \left .\mathcal{O}( \| f(X_2) - f(X_1)\|^4 ) \right)^2 \\
&=& \sum_{ijk} g_j^i (f(X_1)) g_k^i (f(X_1)) (f^j(X_2) - f^j(X_1) ) (f^k(X_2) - f^k(X_1) ) \\
&& + \sum_{ijkl} g_j^i (f(X_1)) g^i_{kl} (f(X_1)) (f^j(X_2) - f^j(X_1) )  (f^k(X_2) - f^k(X_1))(f^l(X_2) - f^l(X_1)) \\
&& + \frac{1}{4} \sum_{ijklm}  g^i_{jk} (f(X_1)) g^i_{lm} (f(X_1)) (f^j(X_2) - f^j(X_1)) (f^k(X_2) - f^k(X_1)) (f^l(X_2) - f^l(X_1)) (f^m(X_2) - f^m(X_1)) \\
&& + \frac{1}{3} \sum_{ijklm}  g^i_{j} (f(X_1)) g^i_{klm} (f(X_1)) (f^j(X_2) - f^j(X_1)) (f^k(X_2) - f^k(X_1)) (f^l(X_2) - f^l(X_1)) (f^m(X_2) - f^m(X_1)) \\
&& + \mathcal{O} (\|f(X_1) - f(X_2) \|^5 )
\end{eqnarray}

Expanding around $X_2$ yields
\begin{eqnarray}
&&\| X_2 - X_1 \|^2 \\
&=&  \sum_i (X_2^i - X_1^i)^2 \\
&=& \sum_{ijk} g_j^i (f(X_2)) g_k^i (f(X_2)) (f^j(X_2) - f^j(X_1) ) (f^k(X_2) - f^k(X_1) ) \\
&& - \sum_{ijkl} g_j^i (f(X_2)) g^i_{kl} (f(X_2)) (f^j(X_2) - f^j(X_1) )  (f^k(X_2) - f^k(X_1))(f^l(X_2) - f^l(X_1)) \\
&& + \frac{1}{4} \sum_{ijklm}  g^i_{jk} (f(X_2)) g^i_{lm} (f(X_2)) (f^j(X_2) - f^j(X_1)) (f^k(X_2) - f^k(X_1)) (f^l(X_2) - f^l(X_1)) (f^m(X_2) - f^m(X_1)) \\
&& + \frac{1}{3} \sum_{ijklm}  g^i_{j} (f(X_2)) g^i_{klm} (f(X_2)) (f^j(X_2) - f^j(X_1)) (f^k(X_2) - f^k(X_1)) (f^l(X_2) - f^l(X_1)) (f^m(X_2) - f^m(X_1)) \\
&& + \mathcal{O} (\|f(X_1) - f(X_2) \|^5 )
\end{eqnarray}

Averaging the two expansions yields
\begin{eqnarray}
&&\| X_2 - X_1 \|^2 \\
&=&  \sum_i (X_2^i - X_1^i)^2 \\
&=& \frac{1}{2} \sum_{ijk} \left( g_j^i (f(X_1)) g_k^i (f(X_1)) + g_j^i (f(X_2)) g_k^i (f(X_2))\right)   (f^j(X_2) - f^j(X_1) ) (f^k(X_2) - f^k(X_1) ) \\
&& + \frac{1}{2} \sum_{ijkl} \left( g_j^i (f(X_1)) g^i_{kl} (f(X_1)) - g_j^i (f(X_2)) g^i_{kl} \right) (f(X_2)) (f^j(X_2) - f^j(X_1) )  (f^k(X_2) - f^k(X_1))(f^l(X_2) - f^l(X_1)) \\
&& + \frac{1}{8} \sum_{ijklm}  \left( g^i_{jk} (f(X_1)) g^i_{lm} (f(X_1)) + g^i_{jk} (f(X_2)) g^i_{lm} (f(X_2)) \right) (f^j(X_2) - f^j(X_1)) (f^k(X_2) - f^k(X_1)) (f^l(X_2) - f^l(X_1)) (f^m(X_2) - f^m(X_1)) \\
&& + \frac{1}{6} \sum_{ijklm} \left( g^i_{j} (f(X_1)) g^i_{klm} (f(X_1)) + g^i_{j} (f(X_2)) g^i_{klm} (f(X_2)) \right) (f^j(X_2) - f^j(X_1)) (f^k(X_2) - f^k(X_1)) (f^l(X_2) - f^l(X_1)) (f^m(X_2) - f^m(X_1)) \\
&& + \mathcal{O} (\|f(X_1) - f(X_2) \|^6 ) \\
&=& \frac{1}{2} \sum_{ijk}\left( g_j^i (f(X_1)) g_k^i (f(X_1)) + g_j^i (f(X_2)) g_k^i (f(X_2))\right)  (f^j(X_2) - f^j(X_1) ) (f^k(X_2) - f^k(X_1) ) \\
&& + \frac{1}{2} \sum_{ijkl} \left( g_j^i (f(X_1)) g^i_{kl} (f(X_1)) - g_j^i (f(X_2)) g^i_{kl} \right) (f(X_2)) (f^j(X_2) - f^j(X_1) )  (f^k(X_2) - f^k(X_1))(f^l(X_2) - f^l(X_1)) \\
&& +  \sum_{ijklm}  \left( \frac{1}{8} \left( g^i_{jk} (f(X_1)) g^i_{lm} (f(X_1)) + g^i_{jk} (f(X_2)) g^i_{lm} (f(X_2)) \right) + \frac{1}{6} \left( g^i_{j} (f(X_1)) g^i_{klm} (f(X_1)) + g^i_{j} (f(X_2)) g^i_{klm} (f(X_2)) \right) \right) (f^j(X_2) - f^j(X_1)) (f^k(X_2) - f^k(X_1)) (f^l(X_2) - f^l(X_1)) (f^m(X_2) - f^m(X_1)) \\
&& + \mathcal{O} (\|f(X_1) - f(X_2) \|^6 )
\end{eqnarray}



\begin{itemize}
\item Make 2D with fast-slow
\item Use $dt$ rather than $\sigma$ for variance
\item Add into Mahalanobis distance approximation
\item Add second order term in Mahalanobis distance
\end{itemize}
\end{document}